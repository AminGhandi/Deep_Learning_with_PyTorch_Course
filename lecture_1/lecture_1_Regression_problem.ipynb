{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "from plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "\n",
    "#model: y = b + w x + epsilon\n",
    "y = true_b + true_w * x + epsilon\n",
    "\n",
    "# split data to train_test and Shuffle\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2,shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will be using batch gradient descent only, meaning, we’ll use all data\n",
    "points for each one of the four steps above. It also means that going once through\n",
    "all of the steps is already one epoch. Then, if we want to train our model over 1,000\n",
    "epochs, we just need to add a single loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① Step 0: Random initialization of parameters / weights\n",
    "\n",
    "② Initialization of hyper-parameters\n",
    "\n",
    "③ Step 1: Forward pass\n",
    "\n",
    "④ Step 2: Computing loss\n",
    "\n",
    "⑤ Step 3: Computing gradients\n",
    "\n",
    "⑥ Step 4: Updating parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n",
      "[1.01429206] [1.95986328]\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "print(b, w)\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient\n",
    "    # descent. How wrong is our model? That's the error!   \n",
    "    error = (yhat - y_train)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    b_grad = 2 * error.mean()\n",
    "    w_grad = 2 * (x_train * error).mean()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and \n",
    "    # the learning rate\n",
    "    b = b - lr * b_grad\n",
    "    w = w - lr * w_grad\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "point: There are ways of\n",
    "stopping it earlier, once the progress is considered negligible (for instance, if the\n",
    "loss was barely reduced). These are called, most appropriately, early stopping\n",
    "methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.01429133] [1.95986473]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: do we get the same results as our\n",
    "# gradient descent?\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = figure3(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "**A scalar (a single number) has zero dimensions, a vector has one\n",
    "dimension, a matrix has two dimensions, and a tensor has three\n",
    "or more dimensions.**\n",
    "\n",
    "But, to keep things simple, it is commonplace to call vectors and matrices tensors as\n",
    "well—so, from now on, everything is either a scalar or a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1416)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[ 1.2189, -0.7499,  1.1768,  0.1267],\n",
      "         [ 1.1126, -1.0030, -0.5765, -0.2187],\n",
      "         [-0.3212,  0.7264,  1.3247,  0.7378]],\n",
      "\n",
      "        [[-0.4538,  1.2395, -1.8186, -0.2005],\n",
      "         [ 0.0404, -0.4431,  2.4759,  0.9298],\n",
      "         [ 0.8922,  1.6280, -0.8904,  1.8739]]])\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "\n",
    "print(scalar)\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.size(), tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(scalar.size(), scalar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also reshape a tensor using its view() (preferred) or reshape() methods.\n",
    "\n",
    "Beware: The view() method only returns a tensor with the\n",
    "desired shape that shares the underlying data with the original\n",
    "tensor—it DOES NOT create a new, independent, tensor!\n",
    "The reshape() method may or may not create a copy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 2., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# We get a tensor with a different shape but it still is\n",
    "# the SAME tensor\n",
    "same_matrix = matrix.view(1, 6)\n",
    "# If we change one of its elements...\n",
    "same_matrix[0, 1] = 2.\n",
    "# It changes both variables: matrix and same_matrix\n",
    "print(matrix)\n",
    "print(same_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to copy all data, that is, duplicate the data in memory, you may use\n",
    "either its new_tensor() or clone() methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 4., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Lets follow PyTorch's suggestion and use \"clone\" method\n",
    "another_matrix = matrix.view(1, 6).clone().detach()\n",
    "# Again, if we change one of its elements...\n",
    "another_matrix[0, 1] = 4.\n",
    "# The original tensor (matrix) is left untouched!\n",
    "print(matrix)\n",
    "print(another_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data, Devices and CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to start converting our Numpy code to PyTorch: We’ll start with the\n",
    "training data; that is, our x_train and y_train arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tensor = torch.as_tensor(x_train)\n",
    "x_train.dtype, x_train_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_tensor = x_train_tensor.float()\n",
    "float_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining your device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only created CPU tensors. What does it mean? It means the data in\n",
    "the tensor is stored in the computer’s main memory and any operations performed\n",
    "on it are going to be handled by its CPU. So, although the data is, technically speaking, in the\n",
    "memory, we’re still calling this kind of tensor a CPU tensor.\n",
    "\n",
    "There is also a GPU tensor. A GPU ) is the processor of a graphics card. These tensors store their data in the graphics card’s memory, and operations on top of them are performed by the GPU.\n",
    "\n",
    "If you have a graphics card from NVIDIA, you can use the power of its GPU to\n",
    "speed up model training. PyTorch supports the use of these GPUs for model\n",
    "training using CUDA (Compute Unified Device Architecture), which needs to be\n",
    "previously installed and configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know how many GPUs it\n",
    "has, or which model they are, you can figure it out using cuda.device_count() and\n",
    "cuda.get_device_name():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce MX150\n"
     ]
    }
   ],
   "source": [
    "n_cudas = torch.cuda.device_count()\n",
    "for i in range(n_cudas):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "turn our tensor into a GPU tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9219], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tensor = torch.as_tensor(x_train).to(device)\n",
    "gpu_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them \n",
    "# into PyTorch's Tensors and then we send them to the \n",
    "# chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we defined a device, converted both Numpy arrays into PyTorch tensors, cast\n",
    "them to floats, and sent them to the device. Let’s take a look at the types:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# Here we can see the difference - notice that .type() is more\n",
    "# useful since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter requires the computation of its gradients, so we can update their values\n",
    "(the parameters’ values, that is). That’s what the requires_grad=True argument is\n",
    "good for. It tells PyTorch to compute gradients for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first chunk of code below creates two tensors for our parameters, including\n",
    "gradients and all. But they are CPU tensors, by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FIRST\n",
    "# Initializes parameters \"b\" and \"w\" randomly, ALMOST as we\n",
    "# did in Numpy since we want to apply gradient descent on\n",
    "# these parameters we need to set REQUIRES_GRAD = TRUE\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am assuming you’d like to use your GPU (or the one from Google Colab), right? So\n",
    "we need to send those tensors to the device. We can try the naive approach, the\n",
    "one that worked well for sending the training data to the device. That’s our second\n",
    "(and failed) attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([0.1288], device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just\n",
    "# send them to device, right?\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(b, w)\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We succeeded in sending them to another device, but we \"lost\" the gradients\n",
    "somehow, since there is no more requires_grad=True\n",
    "\n",
    "In the third chunk, we first send our tensors to the device and then use the\n",
    "requires_grad_() method to set its requires_grad attribute to True in place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], device='cuda:0', requires_grad=True) tensor([0.1288], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# THIRD\n",
    "# We can either create regular tensors and send them to\n",
    "# the device (as we did with our data)\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "b.requires_grad_()\n",
    "w.requires_grad_()\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do better: We can assign tensors to a device at the moment of their\n",
    "creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FINAL\n",
    "# We can specify the device at the moment of creation\n",
    "# RECOMMENDED!\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need\n",
    "to worry about partial derivatives, chain rule, or anything like it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It will compute gradients for all (gradient-requiring)\n",
    "tensors involved in the computation of a given variable.\n",
    "Do you remember the starting point for computing the gradients? It was the loss,\n",
    "as we computed its partial derivatives w.r.t. our parameters. Hence, we need to\n",
    "invoke the backward() method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2 - Computes the loss\n",
    "# We are using ALL data points, so this is BATCH gradient descent\n",
    "# How wrong is our model? That's the error! \n",
    "error = (yhat - y_train_tensor)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "# No more manual computation of gradients! \n",
    "# b_grad = 2 * error.mean()\n",
    "# w_grad = 2 * (x_tensor * error).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which tensors are going to be handled by the backward() method applied to the\n",
    "loss?\n",
    "\n",
    "• b\n",
    "\n",
    "• w\n",
    "\n",
    "• yhat\n",
    "\n",
    "• error\n",
    "\n",
    "We have set requires_grad=True to both b and w, so they are obviously included in\n",
    "the list. We use them both to compute yhat, so it will also make it to the list. Then\n",
    "we use yhat to compute the error, which is also added to the list.\n",
    "\n",
    "Do you see the pattern here? If a tensor in the list is used to compute another\n",
    "tensor, the latter will also be included in the list. Tracking these dependencies is\n",
    "exactly what the dynamic computation graph is doing, as we’ll see shortly.\n",
    "\n",
    "What about x_train_tensor and y_train_tensor? They are involved in the\n",
    "computation too, but we created them as non-gradient-requiring tensors, so\n",
    "backward() does not care about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True True\n",
      "False False\n"
     ]
    }
   ],
   "source": [
    "print(error.requires_grad, yhat.requires_grad, \\\n",
    "      b.requires_grad, w.requires_grad)\n",
    "print(y_train_tensor.requires_grad, x_train_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time we use the gradients to update the parameters, we need to zero the\n",
    "gradients afterward. And that’s what zero_() is good for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.]), tensor([0.]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will be placed *after* Step 4\n",
    "# (updating the parameters)\n",
    "b.grad.zero_(), w.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient\n",
    "    # descent. How wrong is our model? That's the error!\n",
    "    error = (yhat - y_train_tensor)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    # No more manual computation of gradients! \n",
    "    # b_grad = 2 * error.mean()\n",
    "    # w_grad = 2 * (x_tensor * error).mean()   \n",
    "    # We just tell PyTorch to work its way BACKWARDS \n",
    "    # from the specified loss!\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and \n",
    "    # the learning rate. But not so fast...\n",
    "    # FIRST ATTEMPT - just using the same code as before\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # b = b - lr * b.grad\n",
    "    # w = w - lr * w.grad\n",
    "    # print(b)\n",
    "\n",
    "    # SECOND ATTEMPT - using in-place Python assigment\n",
    "    # RuntimeError: a leaf Variable that requires grad\n",
    "    # has been used in an in-place operation.\n",
    "    # b -= lr * b.grad\n",
    "    # w -= lr * w.grad        \n",
    "    \n",
    "    # THIRD ATTEMPT - NO_GRAD for the win!\n",
    "    # We need to use NO_GRAD to keep the update out of\n",
    "    # the gradient computation. Why is that? It boils \n",
    "    # down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():\n",
    "        b -= lr * b.grad\n",
    "        w -= lr * w.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we\n",
    "    # need to tell it to let it go...\n",
    "    b.grad.zero_()\n",
    "    w.grad.zero_()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is what we used in the THIRD ATTEMPT..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"222pt\" height=\"283pt\"\n",
       " viewBox=\"0.00 0.00 222.00 283.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 279)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-279 218,-279 218,4 -4,4\"/>\n",
       "<!-- 1948009403552 -->\n",
       "<g id=\"node1\" class=\"node\"><title>1948009403552</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"139,-31 74,-31 74,-0 139,-0 139,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (80, 1)</text>\n",
       "</g>\n",
       "<!-- 1948009349024 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1948009349024</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 62,-86 62,-67 151,-67 151,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1948009349024&#45;&gt;1948009403552 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>1948009349024&#45;&gt;1948009403552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-66.7943C106.5,-60.0669 106.5,-50.404 106.5,-41.3425\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-41.1932 106.5,-31.1933 103,-41.1933 110,-41.1932\"/>\n",
       "</g>\n",
       "<!-- 1948009347872 -->\n",
       "<g id=\"node3\" class=\"node\"><title>1948009347872</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1948009347872&#45;&gt;1948009349024 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>1948009347872&#45;&gt;1948009349024</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.4974,-121.985C67.6944,-114.227 80.0055,-102.575 89.972,-93.1425\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.4818,-95.5862 97.3389,-86.1703 87.6701,-90.5021 92.4818,-95.5862\"/>\n",
       "</g>\n",
       "<!-- 1948009403392 -->\n",
       "<g id=\"node4\" class=\"node\"><title>1948009403392</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-208 23.5,-208 23.5,-177 77.5,-177 77.5,-208\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1948009403392&#45;&gt;1948009347872 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>1948009403392&#45;&gt;1948009347872</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.916C50.5,-169.221 50.5,-159.688 50.5,-151.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54.0001,-151.249 50.5,-141.249 47.0001,-151.249 54.0001,-151.249\"/>\n",
       "</g>\n",
       "<!-- 1948009347920 -->\n",
       "<g id=\"node5\" class=\"node\"><title>1948009347920</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-141 119,-141 119,-122 208,-122 208,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1948009347920&#45;&gt;1948009349024 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>1948009347920&#45;&gt;1948009349024</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.342,-121.985C145.999,-114.227 133.468,-102.575 123.323,-93.1425\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.531,-90.4166 115.825,-86.1703 120.765,-95.543 125.531,-90.4166\"/>\n",
       "</g>\n",
       "<!-- 1948009348208 -->\n",
       "<g id=\"node6\" class=\"node\"><title>1948009348208</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-202 113,-202 113,-183 214,-183 214,-202\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-190\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1948009348208&#45;&gt;1948009347920 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>1948009348208&#45;&gt;1948009347920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-182.794C163.5,-174.602 163.5,-162.058 163.5,-151.547\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-151.241 163.5,-141.241 160,-151.241 167,-151.241\"/>\n",
       "</g>\n",
       "<!-- 1947962240672 -->\n",
       "<g id=\"node7\" class=\"node\"><title>1947962240672</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-275 136.5,-275 136.5,-244 190.5,-244 190.5,-275\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-251\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1947962240672&#45;&gt;1948009348208 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>1947962240672&#45;&gt;1948009348208</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-243.75C163.5,-234.389 163.5,-222.192 163.5,-212.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-212.018 163.5,-202.018 160,-212.019 167,-212.018\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1c58e67bd60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "\n",
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2 - Computes the loss\n",
    "# We are using ALL data points, so this is BATCH gradient\n",
    "# descent. How wrong is our model? That's the error! \n",
    "error = (yhat - y_train_tensor)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# We can try plotting the graph for any python variable: \n",
    "# yhat, error, loss...\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a closer look at its components:\n",
    "\n",
    "• blue boxes ((1)s): these boxes correspond to the tensors we use as\n",
    "parameters, the ones we’re asking PyTorch to compute gradients for\n",
    "\n",
    "• gray boxes (MulBackward0 and AddBackward0): Python operations that involve\n",
    "gradient-computing tensors or its dependencies\n",
    "\n",
    "• green box ((80, 1)): the tensor used as the starting point for the computation\n",
    "of gradients (assuming the backward() method is called from the variable used\n",
    "to visualize the graph)—they are computed from the bottom-up in a graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we’ve been manually updating the parameters using the computed\n",
    "gradients. That’s probably fine for two parameters, but what if we had a whole lot\n",
    "of them? We need to use one of PyTorch’s optimizers, like SGD, RMSprop, or\n",
    "Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many optimizers: SGD is the most basic of them, and\n",
    "Adam is one of the most popular.\n",
    "\n",
    "Different optimizers use different mechanics for updating the\n",
    "parameters, but they all achieve the same goal through, literally,\n",
    "different paths.\n",
    "\n",
    "Remember, the choice of mini-batch size influences the path of\n",
    "gradient descent, and so does the choice of an optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step / zero_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimizer takes the parameters we want to update, the learning rate we want\n",
    "to use (and possibly many other hyper-parameters as well!), and performs the\n",
    "updates through its step() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimizer takes the parameters we want to update, the learning rate we want\n",
    "to use (and possibly many other hyper-parameters as well!).\n",
    "\n",
    "Besides, we also don’t need to zero the gradients one by one anymore. We just\n",
    "invoke the optimizer’s zero_grad() method, and that’s it!\n",
    "\n",
    "In the code below, we create a stochastic gradient descent (SGD) optimizer to update\n",
    "our parameters b and w.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0143], device='cuda:0', requires_grad=True) tensor([1.9599], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient \n",
    "    # descent. How wrong is our model? That's the error! \n",
    "    error = (yhat - y_train_tensor)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and \n",
    "    # the learning rate. No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     b -= lr * b.grad\n",
    "    #     w -= lr * w.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling Pytorch to let gradients go!\n",
    "    # b.grad.zero_()\n",
    "    # w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tackle the loss computation. As expected, PyTorch has us covered once\n",
    "again. There are many loss functions to choose from, depending on the task at\n",
    "hand. \n",
    "\n",
    "Since ours is a regression, we are using the mean squared error (MSE) as loss,\n",
    "and thus we need PyTorch’s nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that nn.MSELoss() is NOT the loss function itself: We do not pass\n",
    "predictions and labels to it! Instead, as you can see, it returns another function,\n",
    "which we called loss_fn: That is the actual loss function. So, we can pass a\n",
    "prediction and a label to it and get the corresponding loss value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0143], device='cuda:0', requires_grad=True) tensor([1.9599], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like\n",
    "# Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # No more manual loss!\n",
    "    # error = (yhat - y_train_tensor)\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"b\" and \"w\" real parameters of the model,\n",
    "        # we need to wrap them with nn.Parameter\n",
    "        self.b = nn.Parameter(torch.randn(1,\n",
    "                                          requires_grad=True, \n",
    "                                          dtype=torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1, \n",
    "                                          requires_grad=True,\n",
    "                                          dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.b + self.w * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([1.0143], device='cuda:0')), ('w', tensor([1.9599], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like\n",
    "# Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters \n",
    "# (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # What is this?!?\n",
    "\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear model\n",
    "        # with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call\n",
    "        self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.7645]], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([0.8300], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dummy = MyLinearRegression().to(device)\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[0.7645]], device='cuda:0')),\n",
       "             ('linear.bias', tensor([0.8300], device='cuda:0'))])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For straightforward models that use a series of built-in PyTorch models (like\n",
    "Linear), where the output of one is sequentially fed as an input to the next, we can\n",
    "use a, er … Sequential model.\n",
    "\n",
    "In our case, we would build a sequential model with a single argument; that is, the\n",
    "Linear model we used to train our linear regression.\n",
    "\n",
    "We’ve been talking about models inside other models. This may get confusing real\n",
    "quick, so let’s follow convention and call any internal model a layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Linear model can be seen as a layer in a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[ 0.4414,  0.4792, -0.1353],\n",
       "                      [ 0.5304, -0.1265,  0.1165],\n",
       "                      [-0.2811,  0.3391,  0.5090],\n",
       "                      [-0.4236,  0.5018,  0.1081],\n",
       "                      [ 0.4266,  0.0782,  0.2784]], device='cuda:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-0.0815,  0.4451,  0.0853, -0.2695,  0.1472], device='cuda:0')),\n",
       "             ('1.weight',\n",
       "              tensor([[-0.2060, -0.0524, -0.1816,  0.2967, -0.3530]], device='cuda:0')),\n",
       "             ('1.bias', tensor([-0.2062], device='cuda:0'))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Building the model from the figure above\n",
    "model = nn.Sequential(nn.Linear(3, 5), nn.Linear(5, 1)).to(device)\n",
    "\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (layer1): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (layer2): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Building the model from the figure above\n",
    "model = nn.Sequential()\n",
    "model.add_module('layer1', nn.Linear(3, 5))\n",
    "model.add_module('layer2', nn.Linear(5, 1))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are MANY different layers that can be used in PyTorch:\n",
    "\n",
    "• Convolution Layers\n",
    "\n",
    "• Pooling Layers\n",
    "\n",
    "• Padding Layers\n",
    "\n",
    "• Non-linear Activations\n",
    "\n",
    "• Normalization Layers\n",
    "\n",
    "• Recurrent Layers\n",
    "\n",
    "• Transformer Layers\n",
    "\n",
    "• Linear Layers\n",
    "\n",
    "• Dropout Layers\n",
    "\n",
    "• Sparse Layers (embeddings)\n",
    "\n",
    "• Vision Layers\n",
    "\n",
    "• DataParallel Layers (multi-GPU)\n",
    "\n",
    "• Flatten Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to put it all together and organize our code into three fundamental parts,\n",
    "namely:\n",
    "\n",
    "• data preparation (not data generation!)\n",
    "\n",
    "• model configuration\n",
    "\n",
    "• model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them\n",
    "# into PyTorch's Tensors and then we send them to the \n",
    "# chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is redundant now, but it won't be when we introduce\n",
    "# Datasets...\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters \n",
    "# (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Sets model to TRAIN mode\n",
    "    model.train()\n",
    "\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and \n",
    "    # the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9599]], device='cuda:0')), ('0.bias', tensor([1.0143], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
